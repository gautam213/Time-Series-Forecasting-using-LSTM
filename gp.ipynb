{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "\n",
    "from deap import base, creator, gp, tools\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "def protectedDiv(x, y): return 0 if (y==0) else (x/y)\n",
    "\n",
    "\n",
    "def codeFunction(*args):\n",
    "    return args\n",
    "\n",
    "\n",
    "def CzekanowskiDistance(u, v):\n",
    "    uv = np.matrix([u, v])\n",
    "    s=np.sum(uv)\n",
    "    uv = np.min(uv, axis=0)\n",
    "    num = 2*np.sum(uv)\n",
    "    den = np.sum(u) + np.sum(v)\n",
    "    return 1.0 - 1.0*num/den\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def FitnessEvaluation(train_samples, feature_size, window_size, toolbox, individual):\n",
    "    \"\"\"Individual fitness evaluation. Based on the classification capabilities.\"\"\"\n",
    "    kNClasses = len(train_samples)\n",
    "    kNInstances = len(train_samples[1])\n",
    "\n",
    "    #>Generate lambda expression of individual being evaluated\n",
    "    ind_lambda = toolbox.compile(expr=individual)\n",
    "\n",
    "    #>Compute feature vectors for all the images in the training set based on\n",
    "    #  current individual lambda and store the results on \"train_set\".\n",
    "    train_set = InstancesFeatures(kNClasses*kNInstances, feature_size)\n",
    "    train_set.populate(ind_lambda, train_samples, window_size)\n",
    "\n",
    "    #>Compute pdist and label each individual using 1NN\n",
    "    D = squareform(pdist(train_set.featuresMatrix().transpose()))\n",
    "    D_cze = squareform(pdist(train_set.featuresMatrix().transpose(),CzekanowskiDistance))\n",
    "\n",
    "    #>Classify sampled instances using 1NN and computes cluster distances\n",
    "    db = 0.0\n",
    "    dw = 0.0\n",
    "    for i in range(0, len(D)):\n",
    "        #>Accuracy (fitness term)\n",
    "        dists_z = D[i]\n",
    "        dists = np.delete(dists_z, i)\n",
    "        min_idx = np.argmin(dists)\n",
    "        # Correct the index shift due to removal of self distance\n",
    "        if (min_idx >= i):\n",
    "            min_idx = min_idx + 1\n",
    "        # Compute and store individual label\n",
    "        label_idx = min_idx // kNInstances\n",
    "        train_set.labelInstance(i, train_samples[label_idx][0])\n",
    "\n",
    "        #>Distance (fitness term)\n",
    "        dists_cze_z = D_cze[i]\n",
    "        mod_i = i // kNInstances\n",
    "        # Separates the distances in blocks from same class and others\n",
    "        d_same = dists_cze_z[mod_i*kNInstances:(mod_i+1)*kNInstances]\n",
    "        d_diff = np.delete(dists_cze_z, list(range(mod_i*kNInstances, (mod_i+1)*kNInstances)))\n",
    "        # Update the distance counters       \n",
    "        db = db + np.min(d_diff)\n",
    "        dw = dw + np.max(d_same)\n",
    "\n",
    "    db = db/(kNClasses*kNInstances)\n",
    "    dw = dw/(kNClasses*kNInstances)\n",
    "\n",
    "    # print 'End of Classification:'\n",
    "    # print train_set.correctClassifications()\n",
    "\n",
    "    accuracy = train_set.correctClassifications()[1]\n",
    "    distance = 1.0/(1 + np.exp(-5.0*(db - dw)))\n",
    "\n",
    "    fitness = 1.0 - (accuracy + distance)/2.0\n",
    "\n",
    "    return (fitness, )\n",
    "\n",
    "\n",
    "def FitnessEvaluationMod(train_samples, feature_size, window_size, toolbox,\n",
    "                        individual):\n",
    "    \"\"\"Individual fitness evaluation. Based on the classification capabilities.\"\"\"\n",
    "    kTreeMinDepth = 2\n",
    "    kTreeMaxDepth = 7\n",
    "    max_len = (2**kTreeMaxDepth-1)*math.log(feature_size,2) + 1\n",
    "    min_len = (2**kTreeMinDepth-1)*math.log(feature_size,2) + 1\n",
    "\n",
    "\n",
    "    kNClasses = len(train_samples)\n",
    "    kNInstances = len(train_samples[1])\n",
    "\n",
    "    #>Generate lambda expression of individual being evaluated\n",
    "    ind_lambda = toolbox.compile(individual)\n",
    "\n",
    "    #>Compute feature vectors for all the images in the training set based on\n",
    "    #  current individual lambda and store the results on \"train_set\".\n",
    "    train_set = InstancesFeatures(kNClasses*kNInstances, feature_size)\n",
    "    train_set.populate(ind_lambda, train_samples, window_size)\n",
    "\n",
    "    #>Compute pdist and label each individual using 1NN\n",
    "    D = squareform(pdist(train_set.featuresMatrix().transpose()))\n",
    "    D_cze = squareform(pdist(train_set.featuresMatrix().transpose(), CzekanowskiDistance))\n",
    "\n",
    "    #>Classify sampled instances using 1NN and computes cluster distances\n",
    "    db = 0.0\n",
    "    dw = 0.0\n",
    "    for i in range(0, len(D)):\n",
    "        #>Accuracy (fitness term)\n",
    "        dists_z = D[i]\n",
    "        dists = np.delete(dists_z, i)\n",
    "        min_idx = np.argmin(dists)\n",
    "        # Correct the index shift due to removal of self distance\n",
    "        if (min_idx >= i):\n",
    "            min_idx = min_idx + 1\n",
    "        # Compute and store individual label\n",
    "        label_idx = min_idx // kNInstances\n",
    "        train_set.labelInstance(i, train_samples[label_idx][0])\n",
    "\n",
    "        #>Distance (fitness term)\n",
    "        dists_cze_z = D_cze[i]\n",
    "        mod_i = i // kNInstances\n",
    "        # Separates the distances in blocks from same class and others\n",
    "        d_same = dists_cze_z[mod_i*kNInstances:(mod_i+1)*kNInstances]\n",
    "        d_diff = np.delete(dists_cze_z, list(range(mod_i*kNInstances, (mod_i+1)*kNInstances)))\n",
    "        # Update the distance counters       \n",
    "        db = db + np.min(d_diff)\n",
    "        dw = dw + np.max(d_same)\n",
    "\n",
    "    db = db/(kNClasses*kNInstances)\n",
    "    dw = dw/(kNClasses*kNInstances)\n",
    "\n",
    "    # print 'End of Classification:'\n",
    "    # print train_set.correctClassifications()\n",
    "    \n",
    "\n",
    "    accuracy = train_set.correctClassifications()[1]\n",
    "    distance = 1.0/(1 + np.exp(-5.0*(db - dw)))\n",
    "    density = 1.0*(max_len - len(individual))/(max_len - min_len)\n",
    "\n",
    "    fitness = 1.0 - (accuracy + distance + density)/3.0\n",
    "\n",
    "    return (fitness, )\n",
    "\n",
    "\n",
    "\n",
    "def CreatePrimitiveSet (window_size, code_size):\n",
    "    \"\"\"TODO:\"\"\"\n",
    "    #About primitives:\n",
    "    # input types requires length (use lists)\n",
    "    # input types use list but arguments are seen as separate elements\n",
    "    # return type must be a class.\n",
    "    # return type must be hashable, so lists which are dynamic elements are not allowed.\n",
    "    kCS = code_size\n",
    "    kWS = window_size\n",
    "\n",
    "    pset = gp.PrimitiveSetTyped(\"EID\", itertools.repeat(float, kWS**2), tuple, \"P\")\n",
    "    pset.addPrimitive(codeFunction, [float]*kCS, tuple)\n",
    "    pset.addPrimitive(operator.add, [float, float], float)\n",
    "    pset.addPrimitive(operator.sub, [float, float], float)\n",
    "    pset.addPrimitive(operator.mul, [float, float], float)\n",
    "    pset.addPrimitive(protectedDiv, [float, float], float)\n",
    "\n",
    "    return pset\n",
    "\n",
    "\n",
    "def DefineEvolutionToolbox (primitive_set, training_instances, feature_size, window_size):\n",
    "    \"\"\"TODO: Parameterize this function so it receives the evolution parameters \n",
    "    from file/struct\"\"\"\n",
    "    import math\n",
    "    kTreeMinDepth = 2\n",
    "    kTreeMaxDepth = 10\n",
    "    kTournamentSize = 7\n",
    "\n",
    "    max_len = (2**kTreeMaxDepth-1)*math.log(feature_size,2) + 1\n",
    "    min_len = (2**kTreeMinDepth-1)*math.log(feature_size,2) + 1\n",
    "\n",
    "    creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "    creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMin)\n",
    "\n",
    "    tbox = base.Toolbox()\n",
    "    tbox.register(\"generate_expr\", gp.genHalfAndHalf, pset=primitive_set, \n",
    "                  min_=kTreeMinDepth, max_=kTreeMaxDepth)\n",
    "    tbox.register(\"generate_ind_tree\", tools.initIterate, creator.Individual, \n",
    "                  tbox.generate_expr)\n",
    "    tbox.register(\"generate_population\", tools.initRepeat, list, tbox.generate_ind_tree)\n",
    "    tbox.register(\"compile\", gp.compile, pset=primitive_set)\n",
    "    tbox.register(\"evaluate\", FitnessEvaluation, training_instances, feature_size,\n",
    "                    window_size, tbox )\n",
    "#     tbox.register(\"evaluate\", FitnessEvaluationMod, training_instances, feature_size,\n",
    "#                 window_size, tbox, min_len, max_len )\n",
    "    tbox.register(\"select\", tools.selTournament, tournsize=kTournamentSize)\n",
    "    tbox.register(\"mate\", gp.cxOnePoint)\n",
    "    tbox.register(\"expr_mut\", gp.genFull, min_=1, max_=2, type_=float)\n",
    "    tbox.register(\"mutate\", gp.mutUniform, expr=tbox.expr_mut, pset=primitive_set)\n",
    "    #enforce size constraint over generated individuals\n",
    "    tbox.decorate(\"mate\", gp.staticLimit(key=operator.attrgetter(\"height\"),\n",
    "                                         max_value=kTreeMaxDepth))\n",
    "    tbox.decorate(\"mutate\", gp.staticLimit(key=operator.attrgetter(\"height\"), \n",
    "                                           max_value=kTreeMaxDepth))\n",
    "\n",
    "    return tbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class InstancesFeatures:\n",
    "    def __init__(self, n_instances, n_features):\n",
    "        self.class_ids_ = np.zeros(shape=n_instances, dtype=int)\n",
    "        self.class_instances_ = np.zeros(shape=n_instances, dtype=int)\n",
    "        self.feature_matrix_ = np.zeros(shape=(n_features, n_instances), dtype=int)\n",
    "        self.label_1nn_ = -1*np.ones(shape=n_instances, dtype=int)\n",
    "\n",
    "    def populate(self, ind_lambda, sample_instances, window_size):\n",
    "        \n",
    "        base_path = r'C:\\\\Kylberg\\\\'\n",
    "        set_idx=0\n",
    "        for s_i in sample_instances:\n",
    "\n",
    "            for inst in s_i[1]:\n",
    "                \n",
    "                str_idx = indexToString(inst)\n",
    "                img_path = base_path + str(s_i[0]) + '\\\\' + str_idx[0]  + str_idx[1] + '.png'\n",
    "                gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                img = cv2.resize(gray,(115,115), interpolation = cv2.INTER_AREA)\n",
    "                #TODO: talvez normalizar imagem de entrada.\n",
    "                np.savetxt('f.csv',img,delimiter=',')\n",
    "                # Compute patch feature vector\n",
    "                shape = self.feature_matrix_.shape\n",
    "                fv = FeatureExtraction(img, ind_lambda, shape[0], window_size)\n",
    "                # Fill the feature vector matrix\n",
    "                self.addInstance(set_idx, s_i[0], inst, fv)\n",
    "                set_idx = set_idx + 1\n",
    "\n",
    "                \n",
    "\n",
    "    def addInstance(self, idx, class_id, class_instance, features):\n",
    "        self.class_ids_[idx] = class_id\n",
    "        self.class_instances_[idx] = class_instance\n",
    "        self.feature_matrix_[:,idx] = features\n",
    "\n",
    "    def labelInstance(self, idx, label):\n",
    "        self.label_1nn_[idx] = label\n",
    "\n",
    "    def correctClassifications(self):\n",
    "        labels = np.zeros(shape=len(self.label_1nn_), dtype=int)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(0, len(labels)):\n",
    "            if (self.label_1nn_[i] == -1):\n",
    "                labels[i] = -1\n",
    "            else:\n",
    "                total = total + 1\n",
    "                if self.class_ids_[i] == self.label_1nn_[i]:\n",
    "                    labels[i] = 1\n",
    "                    correct = correct + 1\n",
    "\n",
    "        acc = (1.0*correct)/total\n",
    "\n",
    "        return (labels, acc)\n",
    "\n",
    "    def numInstances(self):\n",
    "        return len(self.class_ids_)\n",
    "\n",
    "    def featuresVector(self, col_idx):\n",
    "        return self.feature_matrix_[:, col_idx]\n",
    "\n",
    "    def featuresMatrix(self):\n",
    "        return self.feature_matrix_\n",
    "\n",
    "\n",
    "def indexToString(inst_idx):\n",
    "    if (inst_idx < 10):\n",
    "        str_idx = '0' + str(inst_idx)\n",
    "    else:\n",
    "        str_idx = str(inst_idx)\n",
    "    \n",
    "    return str_idx\n",
    "\n",
    "def FeatureExtraction(img, individual_lambda, features_len, window_size):\n",
    "    \"\"\"TODO: Add comment\"\"\"\n",
    "    kNF = features_len\n",
    "    kWS = window_size\n",
    "\n",
    "    img_WH = img.shape\n",
    "    height_w = img_WH[0] - kWS//2\n",
    "    width_w = img_WH[1] - kWS//2\n",
    "\n",
    "    features = np.zeros(kNF, dtype=np.int)\n",
    "    # iterate over image pixels to fill the feature vector (histogram) \n",
    "    for r in range(kWS//2, height_w):\n",
    "        for c in range(kWS//2, width_w):\n",
    "            window = LinearWindow(img, kWS, (r,c))\n",
    "            \n",
    "            bs = np.array(individual_lambda(*window))\n",
    "            bs = bs > 0.0\n",
    "            # print bs \n",
    "            \n",
    "            b = 0\n",
    "            for bit in bs:\n",
    "                b = (b << 1) | bit\n",
    "            # print bin\n",
    "\n",
    "            features[b] = features[b] + 1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def LinearWindow(img, window_size, xxx_todo_changeme):\n",
    "    (row, col) = xxx_todo_changeme\n",
    "    WS = window_size\n",
    "    window = img[row-WS//2:row+WS//2+1, col-WS//2:col+WS//2+1]\n",
    "    l=window.reshape(1, WS**2)[0]\n",
    "    m=[np.min(l),np.max(l),np.std(l),np.mean(l)]\n",
    "    stats = random.choices(m, k=WS**2)\n",
    "    return stats\n",
    "    \n",
    "def TESTofScope():\n",
    "    return 0\n",
    "\n",
    "__all__ = ['LinearWindow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: \n",
      "(classes = 10 code = 5 pop = 50 gen = 20 tests_insts = 30)\n",
      "\n",
      "[(3, [69, 26]), (4, [42, 18]), (10, [36, 59]), (15, [42, 63]), (17, [54, 30]), (19, [5, 50]), (21, [4, 57]), (22, [66, 12]), (25, [45, 30]), (26, [63, 69])]\n",
      "\n",
      "Iteration #:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:1: RuntimeWarning: overflow encountered in ubyte_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \t      \t                        fitness                         \t             height            \n",
      "   \t      \t--------------------------------------------------------\t-------------------------------\n",
      "gen\tnevals\tavg     \tmax     \tmin     \tstd     \tavg \tmax\tmin\tstd    \n",
      "0  \t50    \t0.707854\t0.754621\t0.600365\t0.043393\t4.72\t10 \t2  \t2.60799\n",
      "1  \t38    \t0.67256 \t0.752797\t0.550254\t0.0429132\t4.6 \t10 \t2  \t1.76635\n",
      "2  \t44    \t0.6603  \t0.754086\t0.574937\t0.0445226\t4.42\t9  \t2  \t1.28203\n",
      "3  \t43    \t0.648946\t0.753449\t0.500091\t0.0653148\t4.34\t8  \t2  \t1.47797\n",
      "4  \t41    \t0.626251\t0.754068\t0.47363 \t0.0714086\t5.42\t9  \t2  \t1.62592\n",
      "5  \t48    \t0.638256\t0.754415\t0.52431 \t0.0591853\t6.26\t8  \t3  \t1.61009\n",
      "6  \t41    \t0.617115\t0.728206\t0.524505\t0.0521837\t6.2 \t10 \t3  \t1.86548\n",
      "7  \t43    \t0.613055\t0.703979\t0.449995\t0.059327 \t6.86\t10 \t3  \t1.37128\n",
      "8  \t45    \t0.617334\t0.727777\t0.449995\t0.0649456\t7   \t10 \t4  \t1.29615\n",
      "9  \t42    \t0.58867 \t0.703066\t0.448537\t0.0722806\t6.76\t9  \t1  \t1.60698\n",
      "10 \t43    \t0.590525\t0.70089 \t0.448537\t0.0641466\t7.34\t10 \t5  \t0.950999\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from deap import algorithms, gp, tools\n",
    "import numpy as np\n",
    "\n",
    "#>PARAMETERS--------------------------------------------\n",
    "#>Statistics parameters\n",
    "kNRoundsClasses =               1\n",
    "kNRoundsInstances =             15           #15\n",
    "#>Training parameters\n",
    "kNClassesDataset =              28\n",
    "kNTrainingClasses =             10           #10\n",
    "kClassesSize =                  160\n",
    "kNTrainingInstances =           2\n",
    "#>Algorithm parameters:\n",
    "kCodeSize =                     5           #5-7\n",
    "kWindowSize =                   5           #5\n",
    "#>Evolution parameters:\n",
    "kPopSize =                      50           #25\n",
    "kXOverRate =                    0.8\n",
    "kMutRate =                      0.2\n",
    "kElitRate =                     0.01\n",
    "kMaxGenerations =               20           #30\n",
    "\n",
    "kNTestInstances =               30\n",
    "#-------------------------------------------------------\n",
    "\n",
    "def ComputeAccuracyOverTestSet(best_ind, training_instances, n_test_instances, test_base_idx,\n",
    "                                features_len, window_size):\n",
    "    # Transform training instances to feature space\n",
    "    kNClasses = len(training_instances)\n",
    "    kNTrainInstances = len(training_instances[1])\n",
    "\n",
    "\n",
    "    # Compute training features: 1NN base of classification\n",
    "    train_set = InstancesFeatures(kNClasses*kNTrainInstances, features_len)\n",
    "    train_set.populate(best_ind, training_instances, window_size)\n",
    "        \n",
    "    # Create set of test instances \n",
    "    test_instances = []\n",
    "    for t_i in training_instances:\n",
    "        test_instances = test_instances + [(t_i[0], list(range(test_base_idx, test_base_idx + n_test_instances)))]\n",
    "\n",
    "    # Compute the test features\n",
    "    test_set = InstancesFeatures(kNClasses*n_test_instances, features_len)\n",
    "    test_set.populate(best_ind, test_instances, window_size)\n",
    "    test_matrix=test_set.featuresMatrix()\n",
    "    np.savetxt(\"test.csv\",test_matrix.T,delimiter=\",\")\n",
    "\n",
    "    # 1NN classification \n",
    "    train_matrix = train_set.featuresMatrix()\n",
    "    np.savetxt(\"train.csv\",train_matrix.T,delimiter=\",\")\n",
    "    dists = np.zeros(shape=train_set.numInstances())\n",
    "    for i in range(0, test_set.numInstances()):\n",
    "        for j in range(0, train_set.numInstances()):\n",
    "            dists[j] = np.linalg.norm(test_set.featuresVector(i) - train_matrix[:,j])\n",
    "        \n",
    "        min_idx = np.argmin(dists)\n",
    "        label_idx = min_idx // kNTrainInstances\n",
    "        test_set.labelInstance(i, training_instances[label_idx][0])\n",
    "\n",
    "    return test_set.correctClassifications()[1]\n",
    "\n",
    "kFeatureSize = 2**kCodeSize\n",
    "print('Parameters: \\n(classes = ' + str(kNTrainingClasses) + ' code = ' + str(kCodeSize) + ' pop = ' +  str(kPopSize) + ' gen = ' + str(kMaxGenerations) + ' tests_insts = ' + str(kNTestInstances) + ')\\n')\n",
    "\n",
    "classes = list(range(1, kNClassesDataset+1))\n",
    "\n",
    "#>Iterate over classes\n",
    "for n_cl in range(0, kNRoundsClasses):\n",
    "    # Randomize classes to use on experiment\n",
    "    sample_classes = random.sample(classes, kNTrainingClasses)\n",
    "    sample_classes.sort()\n",
    "\n",
    "    # For each selected class, randomize the training instances\n",
    "    sample_instances = []\n",
    "    for i in sample_classes:\n",
    "        sample_instances = sample_instances + \\\n",
    "            [(i, random.sample(list(range(0, kClassesSize//2)), kNTrainingInstances))]\n",
    "#     sample_instances = [(3, [17, 31]), (24, [6, 45]), (9, [5, 15]), (18, [14, 35]), (23, [26, 43]), (19, [4, 45]), (7, [14, 21]), (17, [21, 27]), (10, [23, 37]), (12, [10, 7])]\n",
    "    print(sample_instances)\n",
    "\n",
    "    #>Define Evolution framework\n",
    "    pset = CreatePrimitiveSet(kWindowSize, kCodeSize)\n",
    "    tbox = DefineEvolutionToolbox(pset, sample_instances, kFeatureSize, kWindowSize)\n",
    "\n",
    "    #>Define Log structure\n",
    "    stats_fit = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats_size = tools.Statistics(lambda ind: ind.height)\n",
    "    mstats = tools.MultiStatistics(fitness=stats_fit, height=stats_size)\n",
    "    mstats.register(\"avg\", np.mean)\n",
    "    mstats.register(\"std\", np.std)\n",
    "    mstats.register(\"min\", np.min)\n",
    "    mstats.register(\"max\", np.max)\n",
    "\n",
    "    #>RERUN THE ALGORITHM WITH DIFFERENT SEEDS\n",
    "    accs = np.zeros(shape=kNRoundsInstances) \n",
    "    for n_in in range(0, kNRoundsInstances):\n",
    "        print('\\nIteration #: ', n_in)\n",
    "        # Generate population\n",
    "        pop = tbox.generate_population(kPopSize)\n",
    "        hof = tools.HallOfFame(1)\n",
    "\n",
    "        # Evolutionary algorithm call\n",
    "        pop, log = algorithms.eaSimple(pop, tbox, kXOverRate, kMutRate, kMaxGenerations,\n",
    "                                    stats=mstats, halloffame=hof, verbose=True)\n",
    "        # ACTIVATE TRY CATCH BLOCK AFTER DEBUG IS COMPLETE (AVOID LIBRARY STABILITY ISSUES)\n",
    "        # try: \n",
    "        #     pop, log = algorithms.eaSimple(pop, tbox, kXOverRate, kMutRate, kMaxGenerations,\n",
    "        #                                 stats=mstats, halloffame=hof, verbose=True)\n",
    "        # except:\n",
    "        #     print 'The evolutionary algorithm failed to evolve'\n",
    "        #     continue\n",
    "\n",
    "        print('\\nBest individual:\\n', hof[0])\n",
    "\n",
    "        # fh_log = open('./../results/' + str(n_cl) + '_' + str(n_in) + '.log', 'w')\n",
    "        # fh_hof = open('./../results/' + str(n_cl) + '_' + str(n_in) + '.hof', 'w')\n",
    "        # pickle.dump(log, fh_log)\n",
    "        # pickle.dump(hof[0], fh_hof)\n",
    "        # fh_log.close()\n",
    "        # fh_hof.close()\n",
    "\n",
    "        ind_lambda = tbox.compile(expr=hof[0])\n",
    "        accs[n_in] = ComputeAccuracyOverTestSet(ind_lambda, sample_instances, kNTestInstances,\n",
    "                                             kClassesSize//2, kFeatureSize, kWindowSize)\n",
    "        print('Accuracy = ', accs[n_in])\n",
    "\n",
    "    print('Accuracies = ', accs)\n",
    "    print('Mean = ', np.mean(accs))\n",
    "    print('Std Dev = ', np.std(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
